---
title: "Introduction to Multiscale Geographic Weighted Regressions (MGWR)"
subtitle: ""
author: "Daniel Beene & Theodros Woldeyohannes"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  pdf_document:
    df_print: kable
  html_document:
    toc: true
    number_sections: true
    toc_depth: 5
    code_folding: show
    toc_float: true
    collapsed: false
    smooth_scroll: TRUE
    theme: spacelab
    highlight: tango
fontsize: 12pt
geometry: margin=0.25in
always_allow_html: yes
---

<style>
/* HTML FORMATTING */

h1, .h1, h2, .h2, h3, .h3, h4, .h4, h5, .h5 {
  margin-top: 25px; /* space before each header */
  font-weight: bold; /* bold headers */
}
</style>
----------------------------------------

```{r setup, include=FALSE}
knitr::opts_chunk$set(comment = NA, message = FALSE, warning = FALSE, width = 100)
knitr::opts_chunk$set(fig.align = "center", fig.height = 4, fig.width = 6)
knitr::opts_chunk$set(cache = FALSE)
```

```{r}
# Installing libraries
# If you don't have any of the required packages installed, un-comment and run the necessary lines
# install.packages("rgdal")
# install.packages("GWmodel")
# install.packages("ggplot2")
# install.packages("GGally")
# install.packages("gridExtra")
# install.packages("tmap")
# install.packages("dplyr")
# install.packages("sf")
# install.packages("stringr")
# install.packages("tidyverse")
# install.packages("Hmisc")
# install.packages("reshape2")
# install.packages("car")
# install.packages("ellipse")
# install.packages("leaps")
# install.packages("nortest")

```

```{r}
# Import libraries
library(GWmodel)
library(ggplot2)
library(GGally)
library(gridExtra)
library(tmap)
library(dplyr)
library(sf)
library(stringr)
library(tidyverse)
library(Hmisc)
library(car)
library(nortest)
library(spdep)

# Import plotting functions
source("plotting_functions.R")

```

### BACKGROUND: Geographic Information Systems (GIS) provide versatile tools for uncovering spatial patterns of public health and environmental justice (EJ) issues at multiple scales within the exposome.There are many open-source GIS platforms that allow for cost-effective and equitable integration of GIS tools into data analysis workflows. With open-source GIS, public health researchers can effectively collaborate and easily share reproducible methodologies and findings, helping to foster more inclusive and innovative approaches to cross-cutting health challenges. 

In this workshop, you will be introduced to open-source GIS using R, as well as techniques for leveraging GIS tools and spatial data to advance public health research related to the plural environmental health effects of climate change in EJ communities. By participating in this workshop, you will learn the basics of GIS data-structures, loading and visualizing spatial data, and spatial relationships in regression analyses.

### MODELING: Modeling, at its most simple, is to use of mathematics and statistical assumptions on sample data to make predictions about the real world. Geospatial modeling, as performed through GIS, adds the assumption that spatial patterns in the data and locations of observations allows us to uncover relationships that may otherwise go missed. Geospatial modeling has applications in diverse topics from the analysis of geomorphic features and physical processes to logistics and supply chains. One domain of study that can be enhanced through geospatial modeling is public health and epidemology, in which associations between disease outcomes and predictor variables can be viewed as inherently spatial processes. To understand the nuance of geospatial modeling as it relates to public health, we must first go over the distinction between global and local models.

### GLOBAL MODELS: In the context of spatial statistics, a global model considers the relationships between all dependent and independent variables across the entire study extent, with the assumption that observed relationships hold true at all locations within the extent. The implications of these assumptions are that data from different locations within a model can be used with equal weighting, allowing for the estimation of a single parameter for each relationship being modeled. Global models have been the standard of statistical analysis until relatively recently.  Geographers have pointed out for some time that the assumptions of global models have flaws, especially when examining complex relationships. For example, Simpson's paradox, also known as the ecological fallicy, is when a relationship observed at a certain scale is assumed to hold true at another scale. In other words, a relationship assumed to hold true across an entire study extent may infact vary across localized areas of the extent. This is where local models come into play. 

### LOCAL MODELS: In the context of spatial statistics, a local model considers the relationships between dependent and independent variables (or a sub-set of these variables) across a localized area of the study extent. We refer to these localized areas as 'neighborhoods'. Local models assume that observed relationships only hold within the neighborhood, and may vary in different neighborhoods. This allows for local models to potentially model processes that vary over space with greater accuracy. This is valuable in the realm of public health, in which associations between environmental exposure, human behavior, and disease outcomes are complex and show non-linear, spatially heterogenious relationships. 

### SPATIAL AUTOCORRELATION: Spatial Autocorrelation is when data from locations near one another in space are more likely to be similar (i.e. are more correlated) than data from locations remote from one another. Spatial Autocorrelation can be thought of as a double-edged sword. From a geographic perspective, we expect that raw data will show spatial autocorrelation, and use this to our advantage. For example, there is a notion in geography known as Tobler's Law, in which it is assumed that objects in space closer to each other are more likely to be related than objects that are farther apart. When examining processes that vary over space, we assume that data representing these processes will be non-randomly distributed (i.e. show spatial autocorrelation), and that in assesing these distributions through spatial statistics, we can infer otherwise hidden properties of the process. 

On the other-hand, significant spatial autocorrelation of the residuals of a spatial statistical test indicates that the model we are using is misspecified and poorly calibrated, indicating that we are missing significant spatial relationships in the data. Through this workshop we will show how traditional global-linear regressions miss these spatial relationships, and how such relationships can be better captured through spatial and localized geographically weighted regressions.

### GEOGRAPHICALLY WEIGHTED REGRESSION (GWR): GWR is a spatial regression technique that evaluates a local model of the variable or process you are trying to understand or predict by fitting a regression equation to every feature in the dataset. While GWR is a local model, it does not assess the spatial scale at which processes may vary. In other words it assumes variance across an extent, but that all variances exist at the same scale across neighborhoods.

### MULTISCALE GEOGRAPHICALLY WEIGHTED REGRESSION (MGWR):MGWR builds upon the GWR technique, in which regression coefficients of explanatory variables can vary across space. This allows for the MGWR framework to examine both spatial variability and scalar effects simulatensouly in local modeling. 

### OTHER MODELING APPROACHES: Various other local spatial models exist, such as Bayesian Spatially Varying Coefficients Models, Spatial Filtering methods, and Spatially Clustered Coefficient models. Additionally, other kinds of modeling approaches, such as machine learning based models, can be used to assess spatial relationships in data. This workshop will focus on the MGWR framework, given its computational efficiency, robust support in R, and widespread use. 

### EXAMPLE - Low Birthweight Rates: In this workshop, you will examine rates of low birth weight in the state of Colorado in relation to pollution sources and socio-economic co-variates. Our main objective is to demonstrate how spatial relationships may be lurking variables in regression models. 

### DATA: This workshop uses the below data-sets aggregated to census-tracts across a study extent of the state of Colorado:
*dependent variable* - rate of low birthweight (disease outcome)
*independent variable* - 10-year average (2008 - 2018) of total releases in pounds from Toxics Release Inventory (TRI) facilities (potential environmental exposure)
*co-variates* - potential confounders from the Social Vulnerability Index (SVI): poverty, unemployment, income, graduation rates, age, disability, single parent households, race/ethnicity, English proficiency, housing, transportation, insurance

### GIS DATA-FORMAT: Raw data is imported in the form of spreadsheets, in which observations have locational identifiers. We then put this data into a GIS environment and spatialize it by joining the data to spatial objects. For this workshop we will use vector data. Vector data can simply be thought of as polygons (shapes) with locational information. In other words, areas projected onto the surface of the Earth. For this workshop we will use vector data in *shapefile* format. In the case of this workshop, these shapes represent the census tracts of the state of Colorado. By joining the raw data to census tracts, we can use spatial statistics to analyze the spatial relationships within the data.

### R CHEAT CODES: R is an extensive coding environment. It would be impossible to explain every command in this workshop. The intent of this workshop is to introduce you to performing spatial data-analysis in R, and give you to the background to learn and apply these techniques in your own research. 

Please refer to your cheat sheet for descriptions of common commands and operators. There is also a wealth of community information online. A quick google search can point towards solutions for many problems. Below we have provided basic information on key ideas and commands for this workshop:

*variable* In R we define data-objects, such as a spreadsheet of SVIs, as a variable. Variables allow us to easily reference complex data-objects in code functions.

*<- (assignment operator)* The <- is the assignment operator. This can be though of as an '=' (equals). We use the assignment operator to define variables.

*%>% (pipe operator)* The %>% is the pipe operator. This is a way to chain operations together (i.e. 'pipe' information). It takes the output from the left of the pipe and passes it to an expression on the right. For example, we can use the pipe to take the data-object represented by a variable, and input ('pipe') this into a function.

*Data Frame* - Tables of data in R are referred to as 'data frames'. This is basically a data-object that acts like a spreadsheet, in which columns are variables/fields and rows are observations.



Now that you have some background it is time to get started! Feel free to follow along with us through this workshop, or skip ahead if you dare. If you get stuck at anypoint don't hesistate to let us know, we are here to help! Also remeber to refer to the provided pdf to see what outputs should look like.



#### PART 1: READ IN AND FORMAT DATA
In Part 1, you will import and format data prior to statistical analysis (steps 1 - 5)

###STEP 1: IMPORT SHAPEFILE
Run the below code chunk (green arrow below) to import and plot a shapefile of census tracts in Colorado. We first read in the shapefile and define it as the variable 'dat_tracts' (line XXX). We then plot the shapefile by calling this variable in the 'plot()' function (lines XXX).

```{r}
# Import shapefile
dat_tracts <- st_read(dsn = "./Tracts/tl_2010_08_tract10.shp")

# Plot shapefile with base R - this is just to check that it renders correctly
par(mar=c(0,0,0,0))
plot(dat_tracts, col="#333333", lwd=0.25, border=0, max.plot = 1)

```

### Explain output



### STEP 2: IMPORT POTENTIAL EXPOSURE DATA

Run the below code chunk (green arrow below) to import and format data table. This data table contains the 10-year (2008 - 2018) average of total releases in pounds from TRI facilities by census tract in Colorado. We need first ensure that the census tract ID (GEOID) is the correct length. Some states have a GEOID that begins with 0, Colorado being one of them. The problem is that CSV files created using MS Excel (most are) will remove the leading 0 in numbers. We will first make sure that every GEOID in the table is 11 characters long by padding them with a 0 on the left side.

We first read in the data table and define it as the variable 'dat_tri' (line XXX). We then modify the GEOID field, padding it with 0s on the left side until each GEOID is 11 characters long (line XXX).

```{r}
dat_tri <- read.csv('./Exposure/avgReleases_tract_CO_2008-2018.csv')
# Pad GEOID to 11 characters
dat_tri$GEOID10 <- str_pad(dat_tri$GEOID10, width = 11, side = "left", pad = 0)
```
Copy and paste 'unique(nchar(dat_tri$GEOID10))' (without the quotes) into the console and hit enter. This command returns the number of characters in the GEOID field. If it returns 11, you are all set! (if not let us know!)
All GEOIDs are `r unique(nchar(dat_tri$GEOID10))` characters long. *Note: if this says something other than 11 make sure the chunk above is running correctly.*



### STEP 3: IMPORT SVI CONFOUNDERS

Run the below code chunk (green arrow below) to import and format a data table. This data table contains national SVI data aggreagted to census tracts. We first read in the data table and define it as a variable ('svi'), and format to pad GEOIDs (called 'FIPS' in the SVI dataset) as done in Step 2 (lines XXX). Next, we pipe the svi variable into a filtering function 'filter()', to filter out data that is located with Colorado census tracts, and define this filtered data as a new variable ('svi_sub) (lines XXX). Next, we convert missing values (-999) to NA (line XXX); this will make it easier to exclude missing values from statistical analysis later on. Lastly, we pipe the svi_sub variable into a selection function ('select()'), to select only fields that we want, and overwrite the svi_sub variable with this selection (lines XXX). 

```{r}
svi <- read.csv('./Confounders/SVI_2018_US.csv') # SVI data for all census tracts in US
svi$FIPS <- str_pad(svi$FIPS, width = 11, side = "left", pad = 0) # Pad GEOIDs so that they are all 11 characters long
# unique(nchar(svi$FIPS))

svi_sub <- svi %>%
  filter(ST_ABBR %in% c("CO")) # Subset SVI to only include tracts in Colorado

svi_sub[svi_sub == -999] <- NA # Missing values are coded as -999, here we replace -999 with NA

# Keep percentage, percentile, theme ranking, flag variables, and total population
svi_sub <- svi_sub %>%
  dplyr::select(FIPS, starts_with(c("E_TOTPOP", "EP_", "EPL_", "SPL_", "RPL", "F_")))
```
Copy and paste 'unique(nchar(svi_sub$FIPS))' (without the quotes) into the console and hit enter. This command returns the number of characters in the GEOID field. If it returns 11, you are all set! (if not let us know!)
All GEOIDs in the SVI dataset are `r unique(nchar(svi_sub$FIPS))` characters long. *Note: if this says something other than 11 make sure the chunk above is running correctly.*



### STEP 4: IMPORT LOW BIRTHWEIGHT DATA (health outcome)

Run the below code chunk (green arrow below) to import and format a data table. This data table contains health outcome data aggregated to census tracts. We first read in the data table and define as a variable ('outcome') (line XXX). Next, we select the fields we need (low birthweight and GEOID) and define this selection as a new variable ('lwb') (line XXX). Lastly, we again format to pad the GEOIDs as done in steps 2 and 3 (line XXX).

```{r}
outcome <- read.csv('./Outcome/CDPHE_Composite_Selected_Health_Outcome_Dataset_(Census_Tract).csv')
lwb <- outcome[, c(2, 26:32)] # Subset to only include low birthweight variables and GEOID

lwb$TRACT_FIPS <- str_pad(lwb$TRACT_FIPS, width = 11, side = "left", pad = 0) # Pad GEOIDs so that they are all 11 characters long
```
Copy and paste 'unique(nchar(lwb$TRACT_FIPS))' (without the quotes) into the console and hit enter. This command returns the number of characters in the GEOID field. If it returns 11, you are all set! (if not let us know!)
All GEOIDs in the low birthweight dataset are `r unique(nchar(lwb$TRACT_FIPS))` characters long. *Note: if this says something other than 11 make sure the chunk above is running correctly.*



### STEP 5: JOIN DATA TABLES

Run the below code chunk (green arrow below) to join the formatted data tables of TVI releases (exposure), SVI confounders, and low birthweight (outcome) to our census tract shapefile, creating a combined spatial dataset. This combined dataset will then be ready to be fed into our statistical models. We first convert the shapefile ('dat_tracts') into a normal dataframe, allowing us to perform join operations. We define the converted shapefile as new variable ('dat_full') (line XXX). Then we pipe the census tracts into a series of join functions ('right_join()'), joining our TVI, SVI, and low birtweight data tables to create a combined table. We overwrite dat_full with the new combined data table. (lines XXX). Next, we create a subset of the dataset and define as a new variable 'global_dat' for use in the global, linear regression model (lines XXX). *CHALLENGE: after you finish the workshop, come back to this step and follow instructions on line XXX to investigate how removing potential outliers effects results*. Next we convert dat_full back into spatial format for use in spatial statistical models (line XXX). Lastly, we return a summary of dat_full to make sure data was joined properly (line XXX) (reference the pdf to make sure your summary looks correct!).

```{r}
# Combine data tables

# Join data using the dplyr library - this doesn't work on SpatialPolygonDataFrame data types, so convert first
dat_full <- st_as_sf(dat_tracts)

dat_full <- dat_full %>%
  right_join(dat_tri, by = c("GEOID10" = "GEOID10")) %>% # Exposure
  right_join(lwb, by = c("GEOID10" = "TRACT_FIPS")) %>% # Outcome
  right_join(svi_sub, by = c("GEOID10" = "FIPS")) # Confounders

# Create a subset for global models
global_dat <- dat_full %>%
  select(GEOID10, avgReleasesLb_10yr, LWB_ADJRATE, E_TOTPOP, starts_with("EP_")) # Keep total population estimate (E_TOTPOT) and land area (ALAND10) for later reference
global_dat <- 
  data.frame(global_dat[, 1:20]
             ) 
# %>% # To only keep certain observations, uncomment this line and move the pipe (%>%) up to the line above
#   # Only retain observations we're analyzing
#   slice(
#     -659
#   )

dat_full <- as(na.omit(dat_full), "Spatial") # Convert dat_full back to spatial - we will use this for the spatial models

summary(dat_full)
```



### PART 2: GLOBAL LINEAR MODEL
In Part 2 you will run global linear regressions and analyze model outputs (steps 6 - 10)

### STEP 6: GLOBAL MODEL - LINEAR REGRESSION
Run the below code chunk (green arrow below) to perform a standard linear regression, in which low birthweight is our dependent variable (health outcome), TVI releases is our independent (potential exposure), and SVI variables are confounders/co-variates. We first pass our data (for the linear regression, the non-spatial 'global_dat') to a linear model function ('lm()') and define the output of the model as the variable 'lm_full' (lines XXX). Then we return the results summary of the linear model (line XXX). (reference the pdf to make sure your summary looks correct!). 

```{r, message=FALSE}
# fit full model
lm_full <- lm(LWB_ADJRATE ~ avgReleasesLb_10yr + EP_POV + EP_UNEMP + EP_PCI + EP_NOHSDP + EP_AGE65 + EP_AGE17 + EP_DISABL
                                               + EP_SNGPNT + EP_MINRTY + EP_LIMENG + EP_MUNIT + EP_MOBILE + EP_CROWD
                                               + EP_NOVEH + EP_GROUPQ + EP_UNINSUR
              , data = global_dat)

summary(lm_full)
```
### STEP 7: STEPWISE-BACKWARD SELECTION LINEAR REGRESSION
Run the code chunk below to perform a step-wise backward linear regression, in which variables are removed until an optimized, reduced model is discovered based on lowest AIC. We do this by passing our linear model into a step function ('step()') and defining the output (our optimized model) as a new variable ('lm_final') (lines XXX). As before, we return a results summary (line XXX).(reference the pdf to make sure your summary looks correct!).  

```{r, message=FALSE}
## Backward selection using model AIC
lm_red_AIC <- step(lm_full, direction="backward", test="F")
lm_final <- lm_red_AIC
summary(lm_final)
```
### STEP 8: PLOT CORRELATION MATRIX
Run the below code chunk to create a correlation-matrix plot of the variables from our reduced linear model. We first define a new variable 'vars' as a list of the model variable names (lines XXX). Next, we create a subset data table by selecting from 'global_dat' the list of variables contained in 'vars' (line XXX). Lastly, we pass this data to a plotting function 'ggpairs()' and define the output plot as variable 'p', and print 'p' to display it (lines XXX).

```{r, fig.height = 10, fig.width = 10}
# Plot correlation matrix of variables from the reduced model
vars <- c(
  "avgReleasesLb_10yr", "EP_POV", "EP_UNEMP", "EP_PCI", 
  "EP_NOHSDP", "EP_AGE17", "EP_DISABL", "EP_MINRTY", 
  "EP_GROUPQ", "EP_UNINSUR"
)

# Create a subset of 'global_dat' with selected variables
global_dat_sub <- global_dat[, c("LWB_ADJRATE", vars)]

# Plot the correlation matrix using ggpairs
p <- ggpairs(
  global_dat_sub,
  upper = list(continuous = wrap("points", alpha = 0.2, size = 0.5)),
  lower = list(continuous = "cor")
)

print(p)
```



### STEP 9: MODEL DIAGNOSTICS

Run the code chunk below to plot model diagnostics. We do this by passing our model output into a linear model diagnostic plotting function 'lm_diag_plots' (line XXX).

```{R, fig.height = 3, fig.width = 10}
# plot diagnostics
lm_diag_plots(lm_red_AIC, sw_plot_set = "simpleAV")
```

* Full model diagnostics identify point 659 as being influential, and the Cook's distance vs. leverage plot suggests that it exerts some noteworthy leverage on the overall model. However, the AV plots don't corroborate this conclusion, showing that point 659 doesn't appear to pull the line of best fit significantly in its direction.


### STEP 10: SPATIAL AUTOCORRELATION
Run the code chunk below to test for spatial autocorrelation in the residuals of our linear model. We do this by first selecting the residuals from the model output and defining the residuals as new variable 'lm_resid_col'. Next, we append this residual-variable to our spatial data table (line XXX). Now we are ready to test for spatial autocorrelation. First we define the neighborhood kernel function and weighting scheme (lines XXX). Then we pass these parameters, along with our spatial data table, to a function that tests for spatial autocorrelation 'moran.mc'. (lines XXX) This function performs a Moran's I, which is a standard test for spatial autocorrealtion. The function will return statistics that allow us to infer how spatially autocorrelated the residuals are, and thus how well calibrated and specified our linear model is.

```{r}
# Spatial autocorrelation of global linear model residuals
lm_resid_col <- lm_red_AIC$residuals # Make data table of residuals from lm_red_AIC
dat_full$residuals <- lm_resid_col[as.character(rownames(dat_full@data))] # Append residuals to dat_full by row name

nb <- poly2nb(dat_full, queen = TRUE) # Define neighbors for each polygon
lw <- nb2listw(nb, style = "U", zero.policy = TRUE) # Assign weights to neighbors
moran_lm <- moran.mc(dat_full$residuals
                    , lw
                    , nsim = 999
                    , alternative = "two.sided")
print(rbind("Moran's I statistic: " ,moran_lm$statistic))
print(rbind("P-value of Moran's I statistic: ", moran_lm$p.value)) 

```
The Moran's $I$ statistic of the linear model is `r moran_lm$statistic` and the p-value is `r moran_lm$p.value`. Because the $p-value$ is $<0.05$, we conclude that the residuals are significantly spatially autocorrelated, meaning that there are significant spatial relationships in the data that are not reconciled by the current linear model.



#### PART 3: LOCAL SPATIAL MODEL
In Part 3 you will run local geographically weighted regressions and analyze model outputs (steps XX)

### STEP 11: CALCULATE DISTANCE MATRIX
Run the below code chunk to calculate a distance matrix. A distance matrix is a matrix in which elements correspond to estimates of a pairwise distance between the sequences in a set. This can be simply though of as a measure of the distances between the locations of all of our observations in our data table. 

```{r}
# Calculate distance matrix
DM <- gw.dist(dp.locat = coordinates(dat_full))
```




### STEP 12: CALCULATE BANDWIDTHS 
Run the below code chunk to define the optimal bandwidths for the GWR model. In the case of GWR, the bandwidth refers to as the number of neighborhoods that will be used for each local regression equation. This can be simply thought of as the size, or area, of each local area that will be modeled per each dependent-independent/confounder relationship. 

```{r, echo=FALSE}
# Define optimal bandwidths for GWR
bw <- bw.gwr(LWB_ADJRATE ~ avgReleasesLb_10yr 
                         + EP_POV
                         + EP_UNEMP
                         + EP_PCI
                         + EP_NOHSDP
                         + EP_AGE17
                         + EP_DISABL
                         + EP_MINRTY
                         + EP_GROUPQ
                         + EP_UNINSUR
           , data = dat_full
           , approach = "CV"
           , kernel = "gaussian"
           , adaptive = TRUE
           , longlat = TRUE
           , dMat = DM)
           
```



### STEP 13: RUN GWR
Run to code chunk below to perform a normal geographically weighted regression. We do this by passing our distance matrix ('DM') and bandwidth ('bw') variables, along with our spatial data table, to a GWR function ('gwr.basic()') and defining its output as a new variable 'gwr_results_dat' (lines XXX). We then return the model diagnositcs to see results (line XXX). 

```{r}
# Basic GWR
gwr_results_dat <- gwr.basic(LWB_ADJRATE ~ avgReleasesLb_10yr 
                                         + EP_POV
                                         + EP_UNEMP
                                         + EP_PCI
                                         + EP_NOHSDP
                                         + EP_AGE17
                                         + EP_DISABL
                                         + EP_MINRTY
                                         + EP_GROUPQ
                                         + EP_UNINSUR
                                         + ALAND10
                           , data = dat_full
                           , bw = bw
                           , dMat = DM
                           , kernel = "gaussian"
                           , adaptive = TRUE
                           , longlat = TRUE)

# Model diagnostics
gwr_results_dat$GW.diagnostic
```



### STEP 14: TEST FOR SPATIAL AUTOCORRELATION
Run the code chunk below to test for spatial autocorrelation in model residuals. We do this the same way as with the linear regression model (see step 10).

```{r}
# Spatial autocorrelation of basic GWR residuals
nb <- poly2nb(gwr_results_dat$SDF, queen = TRUE) # Define neighbors for each polygon
lw <- nb2listw(nb, style = "U", zero.policy = TRUE) # Assign weights to neighbors
moran_gwr <- moran.mc(gwr_results_dat$SDF$residual
                    , lw
                    , nsim = 999
                    , alternative = "two.sided")
print(rbind("Moran's I statistic: " ,moran_gwr$statistic))
print(rbind("P-value of Moran's I statistic: ", moran_gwr$p.value)) 

```
The Moran's $I$ statistic of the GWR model is `r moran_gwr$statistic` and the p-value is `r moran_gwr$p.value`. Because the $p-value$ is $<0.05$, we conclude that the residuals are significantly spatially autocorrelated, meaning that there are significant spatial relationships in the data that are not reconciled by the current local model.

### STEP 15: RUN MGWR
Run the below code chunk to perform a multi-scale geographically weighted regression (MGWR). The model is set up the same was as the GWR, using the 'gwr.multiscale()' function. The major difference here is that we do not predefine a distance matrix and bandwidth. The MGWR does this automatically, optimizing distance and bandwidth based on scalar relationships in the data. To do this, MGWR runs though many iterations, eventually discovering an optimized model based on a criterion. This is very computationally intensive and can take several hours to run. For the purposes of this workshop, we will only do 1 iteration. *To see what an optimized model looks like, refer to the pdf if which the model was run through 100 iterations!* If you happen to have a powerful laptop with you, try running this step again after you complete the workshop with more iterations to see how the model improves!

*Note: even when using 1 iteration, the model can take several minutes to run. Start the code chunk below and sit back. If it takes to long, do not fret! Feel free to follow along with us for the final steps and reference to pdf* 

```{r, message=FALSE}
# Multiscale geographically weighted regression
# Documentation: https://search.r-project.org/CRAN/refmans/GWmodel/html/gwr.multiscale.html
mgwr_results_dat_full <- gwr.multiscale(LWB_ADJRATE ~ avgReleasesLb_10yr 
                                               + EP_POV
                                               + EP_UNEMP
                                               + EP_PCI
                                               + EP_NOHSDP
                                               + EP_AGE17
                                               + EP_DISABL
                                               + EP_MINRTY
                                               + EP_GROUPQ
                                               + EP_UNINSUR
                                      , data = dat_full
                                      , max.iterations = 1 # Set max iterations higher to optimize model - it's at 2 now to minimize computational burden
                                      , kernel = "gaussian"
                                      , adaptive = TRUE
                                      , criterion = "CVR"
)

# Model outputs
print(mgwr_results_dat_full)


# Model diagnostics
print(mgwr_results_dat_full$GW.diagnostic)
```


### STEP 16: TEST FOR SPATIAL AUTOCORRELATION
Run the code chunk below to test for spatial autocorrelation in model residuals. We do this the same way as with the previous tests (see step 10 & 14).

```{r}
# Spatial autocorrelation of MGWR residuals
nb <- poly2nb(mgwr_results_dat_full$SDF, queen = TRUE) # Define neighbors for each polygon
lw <- nb2listw(nb, style = "U", zero.policy = TRUE) # Assign weights to neighbors
moran_mgwr <- moran.mc(mgwr_results_dat_full$SDF$residual
                       , lw
                       , nsim = 999
                       , alternative = "two.sided")
print(rbind("Moran's I statistic: " ,moran_mgwr$statistic))
print(rbind("P-value of Moran's I statistic: ", moran_mgwr$p.value)) 

```
The Moran's $I$ statistic of the linear model is `r moran_mgwr$statistic` and the p-value is `r moran_mgwr$p.value`. Because the $p-value$ is $<0.05$, we conclude that the residuals are significantly spatially autocorrelated, meaning that there are significant spatial relationships in the data that are not reconciled by the current local model.


```{r}
#Plot neighborhood sizes (bandwidths)
```



### STEP: 17: GENERATE TABLE OF SUMMARY STATISTICS
Run to code chunk below to generate a table of summary statistics of MGWR regression coefficients. we first define a new variable 'coefficients' as the dataframe of the MGWR results (line XXX). Then we use an apply function ('apply()') to perform a list of statistical functions (interquartile range and standard deviation) on the coefficients variable, and define the output as a new variable 'summary_stats' (lines XXX). Last, we turn the 'summary_stats' variable into a data frame that we can print as a summary table (lines XXX).

```{r}
# Summary statistics of coefficients
coefficients <- data.frame(mgwr_results_dat_full$SDF[, 2:11])
summary_stats <- apply(coefficients, 2, function(x) {
  c(
    min = round(min(x, na.rm = TRUE), 4),
    q1 = round(quantile(x, 0.25, na.rm = TRUE), 4),
    median = round(median(x, na.rm = TRUE), 4),
    q3 = round(quantile(x, 0.75, na.rm = TRUE), 4),
    max = round(max(x, na.rm = TRUE), 4),
    sd = round(sd(x, na.rm = TRUE), 4)
  )
})
summary_table <- data.frame(t(summary_stats))
print(summary_table)
```



### STEP 18: GENERATE BOXPLOTS OF COEFFICIENTS
Run to below code chunk to generate a box-plot of the distribution of MGWR regression coefficients per variable. We first reshape the coefficient data table ysing a melt function ('melt()') and define as new variable 'melt_coefficients' (line XXX). We then pass this variable to a plotting function ('ggplot()'), pass the outputs to variable 'p2', and print 'p2' to display the box-plots (lines XXX).

```{r, fig.width = 10, fig.height = 4}
# Boxplots of coefficients
melt_coefficients <- reshape2::melt(coefficients)

p2 <- ggplot(melt_coefficients, aes(x = fct_reorder(variable, value, .fun = mean), y = value))
p2 <- p2 + geom_boxplot()
p2 <- p2 + labs(x = "", y = "Coefficient") + ggtitle("Boxplots of MGWR Coefficients")
p2 <- p2 + theme(axis.text.x = element_text(angle = 40, hjust = 1))
p2 <- p2 + geom_hline(yintercept = 0, linetype = "dashed", color = "red")

print(p2)

```



### STEP 19: MAKE A MAP OF THE RESULTS
Congradualtions! You have made it to the last step of the workshop. So far, you have formatted data, ran spatial statistical tests, and analyzed model diagnostics. Given that this workshop is based in geographic perspective, the final step is to obviously make a map! Run the code chunk below to generate maps. The code will generate a map of the census tracts of Colorado for each variable in our analysis. In each map, tracts will be shaded depending on the significance of the regrssion coefficients. We do this by creating a series of sub-plots, using a custom plotting function ('plot.gwr.coefs'). We pass MGWR results for each variable to this function, generating a sub-plot that we define as its own variable 'p1m - p11m' (lines XXX). Next we pass these sub-plot variables to a plot arrangement function ('tmap_arrange') that will organize the sub-plots into a neat, combined plot. We define this combined plot as variable 'p_mgwr', and finally print the plot to display it (lines XXX). *note: depending on your computer, the maps may take awhile to generate!*

```{r, fig.width = 8.5, fig.height = 10.5}
# Map coefficients
p1m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "Intercept", mgwr_results_dat_full$SDF$Intercept_TV)
p2m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "avgReleasesLb_10yr", mgwr_results_dat_full$SDF$avgReleasesLb_10yr_TV)
p3m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_POV", mgwr_results_dat_full$SDF$EP_POV_TV)
p4m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_PCI", mgwr_results_dat_full$SDF$EP_PCI_TV)
p5m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_NOHSDP", mgwr_results_dat_full$SDF$EP_NOHSDP_TV)
p6m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_AGE17", mgwr_results_dat_full$SDF$EP_AGE17_TV)
p7m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_DISABL", mgwr_results_dat_full$SDF$EP_DISABL_TV)
p8m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_MINRTY", mgwr_results_dat_full$SDF$EP_MINRTY_TV)
p9m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_GROUPQ", mgwr_results_dat_full$SDF$EP_GROUPQ_TV)
p10m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_UNINSUR", mgwr_results_dat_full$SDF$EP_UNINSUR_TV)
p11m <- plot.gwr.coefs(mgwr_results_dat_full$SDF, "EP_UNEMP", mgwr_results_dat_full$SDF$EP_UNEMP_TV)

p_mgwr <- tmap_arrange(p1m
                     , p2m
                     , p3m
                     , p4m
                     , p5m
                     , p6m
                     , p7m
                     , p8m
                     , p9m
                     , p10m
                     , p11m
                     , ncol = 2)
print(p_mgwr)
```








